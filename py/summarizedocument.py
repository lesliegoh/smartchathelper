import argparse
import PyPDF2

# from nltk.tokenize import sent_tokenize
# from py.class_text_summarization import TextSummarization 

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
from heapq import nlargest

# get arguments
ap = argparse.ArgumentParser()

ap.add_argument("path", help="path to input document to be summarized")
args = vars(ap.parse_args())

filePath = args["path"]

##### pdf document
# read pdf
pdfFileObj = open(filePath, 'rb')
 
# creating a pdf reader object
pdfReader = PyPDF2.PdfFileReader(pdfFileObj)
pages = pdfReader.numPages

pdfText = ""
for page in range(pages):
    pageObj = pdfReader.getPage(page)
    pdfText += pageObj.extractText()

# closing the pdf file object
pdfFileObj.close()
# print(pdfText)

# # create sentences
# sentences = sent_tokenize(pdfText.replace("\n", ""))

# # get chat summary
# ts = TextSummarization()

# chat_summary = ts.generate_summary(sentences, 2)
# print(chat_summary)

def summarize(text, per):
    nlp = spacy.load('en_core_web_sm')
    doc= nlp(text)
    tokens=[token.text for token in doc]
    word_frequencies={}
    for word in doc:
        if word.text.lower() not in list(STOP_WORDS):
            if word.text.lower() not in punctuation:
                if word.text not in word_frequencies.keys():
                    word_frequencies[word.text] = 1
                else:
                    word_frequencies[word.text] += 1
    max_frequency=max(word_frequencies.values())
    for word in word_frequencies.keys():
        word_frequencies[word]=word_frequencies[word]/max_frequency
    sentence_tokens= [sent for sent in doc.sents]
    sentence_scores = {}
    for sent in sentence_tokens:
        for word in sent:
            if word.text.lower() in word_frequencies.keys():
                if sent not in sentence_scores.keys():                            
                    sentence_scores[sent]=word_frequencies[word.text.lower()]
                else:
                    sentence_scores[sent]+=word_frequencies[word.text.lower()]
    select_length=int(len(sentence_tokens)*per)
    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)
    final_summary=[word.text for word in summary]
    summary=''.join(final_summary)
    
    return summary

doc_summary = summarize(pdfText.replace("\n", ""), 0.1)
print(doc_summary)